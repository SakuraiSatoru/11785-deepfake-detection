{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import os\n",
    "\n",
    "import dataloader\n",
    "import MultiModal_Model as mm_models\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from notify_run import Notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__ (self, model, test_loader, critereon, device, notify, run_id, num_epochs):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.critereon = critereon\n",
    "        self.device = device\n",
    "        self.notify = notify\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        self.log_file_path = os.path.join('experiments', self.run_id, 'log.txt')\n",
    "        \n",
    "        self.test_losses = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    \"\"\"\n",
    "        return batch_loss: \n",
    "        batch_corrects: num correct predictions this batch\n",
    "        batch_size: num total samples in this batch\n",
    "    \"\"\"\n",
    "    def _test_batch(self, videos, audios, labels):\n",
    "        videos, audios, labels = videos.to(self.device), audios.to(self.device), labels.to(device)\n",
    "        _, preds = model(videos, audios) # linear output\n",
    "        \n",
    "        # calculate loss\n",
    "        batch_loss = self.critereon(preds, labels).item()\n",
    "        \n",
    "        # calculate acc\n",
    "        _, max_preds = torch.max(preds, 1)\n",
    "        batch_corrects = (max_preds == labels).sum().item()\n",
    "        batch_size = labels.shape[0]\n",
    "        return batch_loss, batch_corrects, batch_size\n",
    "    \n",
    "    def _test_notify_and_log(self, epoch_loss, epoch_acc):\n",
    "        test_str = '[TEST]  Epoch {} Loss: {:.4f} Acc: {:.2f}%'.format(self.curr_epoch, epoch_loss, epoch_acc)\n",
    "        print(test_str)\n",
    "        with open(self.log_file_path, 'a+') as f:\n",
    "            f.write(test_str+\"\\n\")\n",
    "        self.notify.send(\"{} {}\".format(self.run_id, test_str))\n",
    "        \n",
    "    def test_epoch(self, curr_epoch):\n",
    "        self.curr_epoch = curr_epoch\n",
    "        trainer_state = torch.load(os.path.join(\"experiments\", self.run_id, \"trainer-epoch-{}.pkl\".format(curr_epoch)))\n",
    "        self.model.load_state_dict(trainer_state[\"model_state_dict\"])\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        epoch_loss = 0.0   \n",
    "        epoch_corrects = 0\n",
    "        num_samples = 0    \n",
    "        num_batches = 0\n",
    "        \n",
    "        for videos, audios, _, _, labels in tqdm(self.test_loader):\n",
    "            batch_loss, batch_corrects, batch_size = self._test_batch(videos, audios, labels)\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_corrects += batch_corrects\n",
    "            num_samples += batch_size\n",
    "            num_batches += 1\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        epoch_acc = epoch_corrects / num_samples * 100.0\n",
    "        \n",
    "        self._test_notify_and_log(epoch_loss, epoch_acc)\n",
    "        self.test_losses.append(epoch_loss)\n",
    "        self.test_acc.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Dataloader\n",
    "Load data for single frame prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 147516 images \n"
     ]
    }
   ],
   "source": [
    "test_video_dataset = dataloader.get_dataset(dataloader.TEST_JSON_PATH, dataloader.SINGLE_FRAME)\n",
    "test_audio_dataset = dataloader.AudioDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data sizes\n",
      "videos shape: torch.Size([64, 3, 224, 224])\n",
      "audios shape: torch.Size([64, 5, 50])\n",
      "labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "test_loader = dataloader.AVDataLoader(test_video_dataset, test_audio_dataset, batch_size=BATCH_SIZE, shuffle=False, single_frame=True)\n",
    "\n",
    "print(\"Verifying data sizes\")\n",
    "for v, a, _, _, l in test_loader:\n",
    "    print('videos shape:', v.shape) # batch_size*3(channel)*224*224\n",
    "    print('audios shape:', a.shape) # batch_size*5*50(channel)\n",
    "    print('labels shape:', l.shape) # batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"MultiModalSimpleConcatDeepfakeInitFreezeModel\" #\"test\" #\n",
    "num_epochs = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "notify = Notify(endpoint=\"https://notify.run/Dbnkja3hR3rG7MuV\")\n",
    "\n",
    "model = mm_models.MultiModalSimpleConcatModel()\n",
    "critereon = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "tester = ModelTrainer(model, test_loader, critereon, device, notify, run_id, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5041c19aeb476295e229f1732a6b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2305), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST]  Epoch 2 Loss: 0.1907 Acc: 95.48%\n"
     ]
    }
   ],
   "source": [
    "curr_epoch = 2\n",
    "tester.test_epoch(curr_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import os\n",
    "\n",
    "import dataloader\n",
    "# import multimodal_ae_models as mm_ae_models\n",
    "import multimodal_ae_simple_models as mm_ae_simple_models\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from notify_run import Notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, \n",
    "                 optimizer, critereon, scheduler,\n",
    "                 device, \n",
    "                 notify, run_id = \"test\"):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.critereon = critereon\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        \n",
    "        # Saved metrics per epoch\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.curr_epoch = 0 # current epoch\n",
    "        \n",
    "        # Storing, logging and notification\n",
    "        self.run_id = run_id # run_id to define storage location\n",
    "        self.notify = notify\n",
    "        self.log_file_path = os.path.join('experiments', self.run_id, 'log.txt')\n",
    "        self._init_storage()\n",
    "        \n",
    "    def load(self, run_id, curr_epoch):\n",
    "        file_path = os.path.join(\"experiments\", run_id, \"trainer-epoch-{}.pkl\".format(curr_epoch))\n",
    "        print(\"Loading from \"+file_path)\n",
    "        trainer_state_dict = torch.load(file_path)\n",
    "        \n",
    "        self.model.load_state_dict(trainer_state_dict[\"model_state_dict\"])\n",
    "        # self.train_loader\n",
    "        # self.val_loader\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "        optimizer.load_state_dict(trainer_state_dict[\"optimizer_state_dict\"])\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.cuda()\n",
    "        self.optimizer = optimizer\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, threshold=0.0001, verbose=True,\n",
    "                                          patience=2, min_lr=1e-6)\n",
    "        scheduler.load_state_dict(trainer_state_dict[\"scheduler_state_dict\"])\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        # self.critereon\n",
    "        # self.device\n",
    "        \n",
    "        self.train_losses = trainer_state_dict[\"train_losses\"]\n",
    "        self.val_losses = trainer_state_dict[\"val_losses\"]\n",
    "        self.val_acc = trainer_state_dict[\"val_acc\"]\n",
    "        self.curr_epoch = curr_epoch\n",
    "        \n",
    "        self.run_id = run_id\n",
    "        # self.notify\n",
    "        self.log_file_path = os.path.join('experiments', self.run_id, 'log.txt')\n",
    "        self._init_storage()\n",
    "        \n",
    "        \n",
    "    def _init_storage(self):\n",
    "        os.makedirs(\"./experiments/{}\".format(self.run_id), exist_ok=True)\n",
    "        print(\"Saving models and run statistics to ./experiments/%s\" % self.run_id)\n",
    "        \n",
    "    def _train_notify_and_log(self, epoch_loss):\n",
    "        train_str = '[TRAIN]  Epoch %d Loss: %.4f' % (self.curr_epoch, epoch_loss)\n",
    "        print(train_str)\n",
    "        with open(self.log_file_path, 'a+') as f:\n",
    "            f.write(train_str+\"\\n\")\n",
    "        self.notify.send(\"{} {}\".format(self.run_id, train_str))\n",
    "    \n",
    "    # Train one epoch\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for videos, audios, _, _, labels in tqdm(self.train_loader): # ignore lens because single frame\n",
    "            videos, audios, labels = videos.to(self.device), audios.to(self.device), labels.to(self.device)\n",
    "            video_out, audio_out, binary_out, classification_embedding, video_embed, audio_embed = model(videos, audios)\n",
    "            batch_loss = self.model.loss((video_out, audio_out, binary_out), (video_embed, audio_embed, labels))\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += batch_loss.item() # add by average loss over batch\n",
    "            num_batches += 1\n",
    "        epoch_loss /= num_batches # divide by num batches\n",
    "        \n",
    "        self.curr_epoch += 1 # put here for 1 index\n",
    "        self._train_notify_and_log(epoch_loss)\n",
    "        self.train_losses.append(epoch_loss)\n",
    "    \n",
    "    \"\"\"\n",
    "        return batch_loss: \n",
    "        batch_corrects: num correct predictions this batch\n",
    "        batch_size: num total samples in this batch\n",
    "    \"\"\"\n",
    "    def _val_batch(self, videos, audios, labels):\n",
    "        videos, audios, labels = videos.to(self.device), audios.to(self.device), labels.to(device)\n",
    "        video_out, audio_out, binary_out, classification_embedding, video_embed, audio_embed = model(videos, audios)\n",
    "        batch_loss = self.model.loss((video_out, audio_out, binary_out), (video_embed, audio_embed, labels)).item()\n",
    "        \n",
    "        # calculate acc\n",
    "        _, max_preds = torch.max(binary_out, 1)\n",
    "        batch_corrects = (max_preds == labels).sum().item()\n",
    "        batch_size = labels.shape[0]\n",
    "        \n",
    "#         # DEBUG\n",
    "#         if batch_loss < 100 and self.good is None:\n",
    "#             self.good = (labels.cpu(), preds.cpu(), batch_loss)\n",
    "#         if (batch_loss > 100 and self.bad is None):\n",
    "#             self.bad = (videos.cpu(), audios.cpu(), labels.cpu(), preds.cpu(), batch_loss)\n",
    "#             raise NotImplemented\n",
    "        return batch_loss, batch_corrects, batch_size\n",
    "    \n",
    "    def _val_notify_and_log(self, epoch_loss, epoch_acc):\n",
    "        val_str = '[VAL]  Epoch {} Loss: {:.4f} Acc: {:.2f}%'.format(self.curr_epoch, epoch_loss, epoch_acc)\n",
    "        print(val_str)\n",
    "        with open(self.log_file_path, 'a+') as f:\n",
    "            f.write(val_str+\"\\n\")\n",
    "        self.notify.send(\"{} {}\".format(self.run_id, val_str))\n",
    "    \n",
    "    # Validate one epoch\n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        epoch_loss = 0.0   \n",
    "        epoch_corrects = 0\n",
    "        num_samples = 0    \n",
    "        num_batches = 0\n",
    "        \n",
    "        # DEBUG\n",
    "        self.good = None\n",
    "        self.bad = None\n",
    "        for videos, audios, _, _, labels in tqdm(self.val_loader):\n",
    "            batch_loss, batch_corrects, batch_size = self._val_batch(videos, audios, labels)\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_corrects += batch_corrects\n",
    "            num_samples += batch_size\n",
    "            num_batches += 1\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        epoch_acc = epoch_corrects / num_samples * 100.0\n",
    "        self.scheduler.step(epoch_loss)\n",
    "        \n",
    "        self._val_notify_and_log(epoch_loss, epoch_acc)\n",
    "        self.val_losses.append(epoch_loss)\n",
    "        self.val_acc.append(epoch_acc)\n",
    "    \n",
    "    \"\"\"\n",
    "        save model weights\n",
    "    \"\"\"\n",
    "    def save(self):\n",
    "        trainer_state_dict = {\n",
    "            \"model_tostring\": str(self.model),\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_acc': self.val_acc\n",
    "        }\n",
    "        torch.save(trainer_state_dict, os.path.join(\"experiments\", self.run_id, \"trainer-epoch-{}.pkl\".format(self.curr_epoch)))\n",
    "    \n",
    "    \"\"\"\n",
    "        save run statistics (loss and acc)\n",
    "    \"\"\"\n",
    "    def save_run_stats(self):\n",
    "        run_stats = {\n",
    "            \"train_losses\" : self.train_losses,\n",
    "            \"val_losses\" : self.val_losses,\n",
    "            \"val_acc\" : self.acc\n",
    "        }\n",
    "        np.save(os.path.join(\"experiments\", self.run_id, \"run_stats.npy\"), run_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Dataloader\n",
    "Load data for single frame prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 733589 images \n",
      "loaded 137015 images \n"
     ]
    }
   ],
   "source": [
    "train_video_dataset = dataloader.get_dataset(dataloader.TRAIN_JSON_PATH, dataloader.SINGLE_FRAME)\n",
    "train_audio_dataset = dataloader.AudioDataset()\n",
    "val_video_dataset = dataloader.get_dataset(dataloader.VAL_JSON_PATH, dataloader.SINGLE_FRAME)\n",
    "val_audio_dataset = dataloader.AudioDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data sizes\n",
      "videos shape: torch.Size([53, 3, 224, 224])\n",
      "audios shape: torch.Size([53, 5, 50])\n",
      "labels shape: torch.Size([53])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = dataloader.AVDataLoader(train_video_dataset, train_audio_dataset, batch_size=BATCH_SIZE, shuffle=True, single_frame=True)\n",
    "val_loader = dataloader.AVDataLoader(val_video_dataset, val_audio_dataset, batch_size=BATCH_SIZE, shuffle=False, single_frame=True)\n",
    "\n",
    "print(\"Verifying data sizes\")\n",
    "for v, a, _, _, l in train_loader:\n",
    "    print('videos shape:', v.shape) # batch_size*3(channel)*224*224\n",
    "    print('audios shape:', a.shape) # batch_size*5*50(channel)\n",
    "    print('labels shape:', l.shape) # batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = []\n",
    "# for _, _, _, _, l in tqdm(train_loader):\n",
    "#     train_labels.append(l)\n",
    "\n",
    "# print(\"train_labels\", torch.cat(train_labels))\n",
    "# print(torch.cat(train_labels).shape)\n",
    "\n",
    "# val_labels = []\n",
    "# for _, _, _, _, l in tqdm(val_loader):\n",
    "#     val_labels.append(l)\n",
    "\n",
    "# print(\"val_labels\", torch.cat(val_labels))\n",
    "# print(torch.cat(val_labels).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models and run statistics to ./experiments/MultiModalAESimpleDense\n"
     ]
    }
   ],
   "source": [
    "run_id = \"MultiModalAESimpleDense\" #\"test\" #\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "notify = Notify(endpoint=\"https://notify.run/Dbnkja3hR3rG7MuV\")\n",
    "\n",
    "model = mm_ae_simple_models.MultiModalAESimpleDensenetModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "critereon = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, threshold=0.0001, verbose=True,\n",
    "                                                      patience=2, min_lr=1e-6)\n",
    "\n",
    "trainer = ModelTrainer(model, train_loader, val_loader, optimizer, critereon, scheduler, device, notify, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.densenet.load_state_dict(torch.load(\"densenet169.pth\")[\"model_state_dict\"])\n",
    "for param in model.densenet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from experiments/MultiModalAESimpleDense/trainer-epoch-1.pkl\n",
      "Saving models and run statistics to ./experiments/MultiModalAESimpleDense\n"
     ]
    }
   ],
   "source": [
    "run_id = \"MultiModalAESimpleDense\" #\"test\" #\n",
    "curr_epoch = 1\n",
    "trainer.load(run_id, curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # self.bad.append((videos, audios, labels, preds, batch_loss))\n",
    "# # self.bad = (videos.cpu(), audios.cpu(), labels.cpu(), preds.cpu(), batch_loss)\n",
    "\n",
    "# # print(trainer.bad)\n",
    "# model.to(\"cuda\")\n",
    "# print(model(trainer.bad[0][:2].cuda(),trainer.bad[1][:2].cuda()))\n",
    "# print(trainer.bad[3], trainer.bad[2])\n",
    "# print(trainer.critereon(trainer.bad[3], trainer.bad[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f1a271ec4c4eeabf00f5ff85c3b358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f746e3438e5248e5a9bd983c0315d2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11463), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN]  Epoch 2 Loss: 0.1340\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2617472657b145a29d2f8f6f714c4208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAL]  Epoch 2 Loss: 0.0985 Acc: 95.60%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61266cc87654ae0be1537326e21d716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11463), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN]  Epoch 3 Loss: 0.0160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1912f9186ff04234af87db1cb7ef59c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAL]  Epoch 3 Loss: 0.1054 Acc: 95.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc31c1a2d7249238c7dc214667d6796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11463), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN]  Epoch 4 Loss: 0.0141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23c24a942a844c689f2d2a4c72d8868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAL]  Epoch 4 Loss: 0.1047 Acc: 95.42%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba07202728804397a05e89f764028aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11463), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    trainer.train_epoch()\n",
    "    trainer.val_epoch()\n",
    "    trainer.save()\n",
    "trainer.save_run_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
